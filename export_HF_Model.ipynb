{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting HuggingFace Models To ONNX Format #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export some embedding models from HF to ONNX ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\SideProject\\DJL_Experiments-main\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting mxbai-embed-large-v1 embedding model\n",
    "\n",
    "model_name = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.3848, -0.1683,  0.1436,  ..., -0.1220,  0.3842, -0.3292],\n",
      "         [ 0.2530,  0.0114,  0.3862,  ..., -0.4523,  0.2744, -0.3398],\n",
      "         [-0.1343,  0.2003,  0.3286,  ...,  0.0507,  0.9975, -0.5576],\n",
      "         ...,\n",
      "         [ 0.3053, -0.1917,  0.4317,  ..., -0.0707,  0.8447, -0.4580],\n",
      "         [ 0.1495, -0.3793,  0.6162,  ...,  0.2146,  0.7438, -0.1687],\n",
      "         [-0.0203, -0.5344,  0.5132,  ..., -0.1450,  0.4284, -0.6931]]]), pooler_output=tensor([[-0.9023, -0.4622, -0.7371,  ...,  0.3709,  0.8697, -0.7646]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "{'input_ids': array([[ 101, 7592, 5292, 2480, 6820, 2140,  102]]), 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "test_input = \"Hello Hazrul\"\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    print(outputs)\n",
    "\n",
    "\n",
    "input_text = tokenizer(test_input, return_tensors=\"np\")\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_export_path = \"mxbai-embed-large-v1.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    # Provide a tuple of inputs; adjust if your model requires more (like token_type_ids)\n",
    "    (inputs[\"input_ids\"], inputs[\"attention_mask\"]),\n",
    "    \"mxbai_embed_large_v1.onnx\",  # Output ONNX file path\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],  # These names will be used in ONNX\n",
    "    output_names=[\"output\"],  # Modify based on your modelâ€™s actual output(s)\n",
    "    dynamic_axes={\n",
    "         \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "         \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "         \"output\": {0: \"batch_size\"}\n",
    "    },\n",
    "    opset_version=14,  # or a higher version if needed\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
